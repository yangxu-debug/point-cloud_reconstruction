import numpy as np
import cv2
import pandas as pd
import os
import re
from scipy.spatial.transform import Rotation as R

def get_correction_transform():
    """
    è¿”å›žä¸€ä¸ª 4x4 å˜æ¢çŸ©é˜µ T_ACï¼Œè¡¨ç¤ºï¼š
    ä»Ž ARKit ç›¸æœºåæ ‡ç³» â†’ æ ¡æ­£åŽåæ ‡ç³»ï¼ˆç»• X è½´æ—‹è½¬ 180Â°ï¼‰
    ç­‰ä»·äºŽ Swift ä¸­çš„ q_AC = simd_quatf(ix: 1.0, iy: 0.0, iz: 0.0, r: 0.0)
    """
    T_AC = np.eye(4)
    # ç»• X è½´æ—‹è½¬ 180Â°
    T_AC[1,1] = -1  # Y â†’ -Y
    T_AC[2,2] = -1  # Z â†’ -Z
    return T_AC

def read_csv_with_timestamp(csv_path):
    """è¯»å– CSVï¼Œè¿”å›ž timestamp -> row çš„å­—å…¸"""
    df = pd.read_csv(csv_path)
    data = {}
    for _, row in df.iterrows():
        ts = float(row['timestamp'])
        data[ts] = row
    return data

def find_closest_timestamp(target_ts, ts_list, tolerance=0.033):
    """åœ¨ ts_list ä¸­æ‰¾æœ€æŽ¥è¿‘ target_ts çš„æ—¶é—´æˆ³"""
    closest = min(ts_list, key=lambda ts: abs(ts - target_ts))
    if abs(closest - target_ts) < tolerance:
        return closest
    return None

def get_depth_image_raw(depth_path, width, height):
    """åŠ è½½ .raw æ·±åº¦å›¾ï¼ˆfloat32ï¼‰"""
    if not os.path.exists(depth_path):
        return None
    try:
        data = np.fromfile(depth_path, dtype=np.float32)
        if data.size != width * height:
            print(f"âš ï¸ Size mismatch: expected {width * height}, got {data.size}")
            return None
        img = data.reshape((height, width))
        return img  # å•ä½ï¼šç±³ï¼ˆå‡è®¾å·²ä¸ºç±³ï¼‰
    except Exception as e:
        print(f"Error loading depth image {depth_path}: {e}")
        return None

def quaternion_to_rotation_matrix(q):
    w, x, y, z = q[3], q[0], q[1], q[2]
    R = np.array([
        [1 - 2*y*y - 2*z*z, 2*x*y - 2*w*z,     2*x*z + 2*w*y],
        [2*x*y + 2*w*z,     1 - 2*x*x - 2*z*z, 2*y*z - 2*w*x],
        [2*x*z - 2*w*y,     2*y*z + 2*w*x,     1 - 2*x*x - 2*y*y]
    ])
    return R

class RGBVideoLoader:
    def __init__(self, video_path, fps=60):
        self.video_path = video_path
        self.cap = cv2.VideoCapture(video_path)
        if not self.cap.isOpened():
            raise IOError(f"Cannot open video: {video_path}")
        self.fps = fps
        self.frame_count = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        print(f"Video loaded: {self.frame_count} frames at {fps} FPS")

    def get_frame_by_timestamp(self, timestamp):
        frame_idx = int(round(timestamp * self.fps))
        if frame_idx < 0 or frame_idx >= self.frame_count:
            return None
        self.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = self.cap.read()
        if not ret:
            return None
        # æ—‹è½¬ 90Â°ï¼ˆæ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
        #frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)
        return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    def release(self):
        if self.cap.isOpened():
            self.cap.release()

# -------------------------------
# ðŸ§© æ•°æ®é¢„åŠ è½½ï¼ˆä¸»å…¥å£ï¼‰
# -------------------------------
def main():
    base_dir = '20250820-212141'  # ä¿®æ”¹ä¸ºä½ è‡ªå·±çš„è·¯å¾„
    depth_dir = os.path.join(base_dir, 'depth')
    extrinsics_file = os.path.join(base_dir, 'extrinsics_log.csv')
    intrinsics_file = os.path.join(base_dir, 'intrinsics_log.csv')
    rgb_video = os.path.join(base_dir, 'video.mov')

    # âš™ï¸ å‚æ•°
    target_size = (256, 192)
    original_rgb_size = (1920, 1440)
    fps = 60
    tolerance = 0.033
    display_width = 960
    display_height = int(1440 * (display_width / 1920))

    T_AC = get_correction_transform()
    # åæ ‡æ˜ å°„æ¯”ä¾‹
    scale_display_to_rgb_x = original_rgb_size[0] / display_width
    scale_display_to_rgb_y = original_rgb_size[1] / display_height
    scale_rgb_to_depth_x = target_size[0] / original_rgb_size[0]
    scale_rgb_to_depth_y = target_size[1] / original_rgb_size[1]

    # -------------------------------
    # ðŸ“‚ 1. åŠ è½½æ‰€æœ‰ .raw æ·±åº¦å›¾ï¼ˆæ—¶é—´æˆ³ â†’ è·¯å¾„ï¼‰
    # -------------------------------
    depth_files = {}
    for f in os.listdir(depth_dir):
        match = re.match(r'^(\d+\.?\d*)_W(\d+)_H(\d+)_DepthFloat32\.raw$', f)
        if match:
            ts = float(match.group(1))
            width = int(match.group(2))
            height = int(match.group(3))
            depth_files[ts] = {
                'path': os.path.join(depth_dir, f),
                'width': width,
                'height': height
            }
    if not depth_files:
        print("âŒ No valid .raw depth files found.")
        return
    print(f"Found {len(depth_files)} depth .raw files")

    # -------------------------------
    # ðŸ“„ 2. åŠ è½½å†…å‚ä¸Žå¤–å‚
    # -------------------------------
    intrinsics_dict = read_csv_with_timestamp(intrinsics_file)
    extrinsics_dict = read_csv_with_timestamp(extrinsics_file)

    # -------------------------------
    # ðŸŽ¥ 3. åŠ è½½è§†é¢‘
    # -------------------------------
    rgb_loader = RGBVideoLoader(rgb_video, fps=fps)

    # -------------------------------
    # ðŸ” 4. æ—¶é—´æˆ³å¯¹é½ â†’ æž„å»ºåŒæ­¥å¸§åˆ—è¡¨
    # -------------------------------
    synced_frames = []
    for depth_ts in sorted(depth_files.keys()):
        closest_intrinsic_ts = find_closest_timestamp(depth_ts, list(intrinsics_dict.keys()), tolerance)
        closest_extrinsic_ts = find_closest_timestamp(depth_ts, list(extrinsics_dict.keys()), tolerance)
        if closest_intrinsic_ts and closest_extrinsic_ts:
            synced_frames.append({
                'timestamp': depth_ts,
                'depth_info': depth_files[depth_ts],
                'intrinsic_row': intrinsics_dict[closest_intrinsic_ts],
                'extrinsic_row': extrinsics_dict[closest_extrinsic_ts]
            })

    if not synced_frames:
        print("âŒ No synchronized frames found.")
        return
    print(f"âœ… {len(synced_frames)} synchronized frames available.")

    # -------------------------------
    # ðŸ–±ï¸ é¼ æ ‡äº‹ä»¶ & ä¸»çª—å£
    # -------------------------------
    world_points_3d = []
    distance_3d = 0.0
    window_name = "3D Measurement"
    current_frame_idx = 0  # å½“å‰æ˜¾ç¤ºå¸§ç´¢å¼•ï¼ˆåœ¨ synced_frames ä¸­ï¼‰

    def click_event(event, x, y, flags, params):
        nonlocal world_points_3d, distance_3d
        if event == cv2.EVENT_LBUTTONDOWN:
            # æ˜¾ç¤ºåæ ‡ â†’ RGBåæ ‡
            x_rgb = int(x * scale_display_to_rgb_x)
            y_rgb = int(y * scale_display_to_rgb_y)
            # RGBåæ ‡ â†’ æ·±åº¦å›¾åæ ‡
            x_d = int(x_rgb * scale_rgb_to_depth_x)
            y_d = int(y_rgb * scale_rgb_to_depth_y)

            frame_data = synced_frames[current_frame_idx]
            depth_img = get_depth_image_raw(
                frame_data['depth_info']['path'],
                frame_data['depth_info']['width'],
                frame_data['depth_info']['height']
            )
            if depth_img is None or y_d >= depth_img.shape[0] or x_d >= depth_img.shape[1]:
                print("Invalid depth data.")
                return

            z = depth_img[y_d, x_d]
            if z <= 0:
                print("Invalid depth value.")
                return

            # é‡å»ºå†…å‚çŸ©é˜µï¼ˆç¼©æ”¾åˆ° target_sizeï¼‰
            K = np.array([[frame_data['intrinsic_row'][f'm{i}{j}'] for j in range(3)] for i in range(3)])
            orig_w, orig_h = original_rgb_size
            target_w, target_h = target_size
            fx = K[0,0] * target_w / orig_w
            fy = K[1,1] * target_h / orig_h
            cx = K[0,2] * target_w / orig_w
            cy = K[1,2] * target_h / orig_h

            X = (x_d - cx) * z / fx
            Y = (y_d - cy) * z / fy
            pt_camera = np.array([X, Y, z])

            # å¤–å‚ï¼šT_world_to_camera
            T_world_to_camera = np.array([[frame_data['extrinsic_row'][f'm{i}{j}'] for j in range(4)] for i in range(4)])
            T_world_to_camera = T_world_to_camera @ T_AC
            R = T_world_to_camera[:3, :3]
            t = T_world_to_camera[:3, 3]
            pt_world = R @ pt_camera + t

            if len(world_points_3d) < 2:
                world_points_3d.append(pt_world)
                print(f"3D Point {len(world_points_3d)}: {pt_world}")
            else:
                world_points_3d[0] = world_points_3d[1]
                world_points_3d[1] = pt_world

            if len(world_points_3d) == 2:
                distance_3d = np.linalg.norm(world_points_3d[1] - world_points_3d[0])
                print(f"3D Distance: {distance_3d:.3f} meters")

    # -------------------------------
    # ðŸŽ¬ ä¸»å¾ªçŽ¯
    # -------------------------------
    cv2.namedWindow(window_name)
    cv2.resizeWindow(window_name, display_width, display_height)
    cv2.createTrackbar('Frame', window_name, 0, len(synced_frames) - 1, lambda x: None)
    cv2.setMouseCallback(window_name, click_event)

    print("æ‹–åŠ¨è¿›åº¦æ¡åˆ‡æ¢å¸§")
    print("ç‚¹å‡»ä¸¤ä¸ªç‚¹å®šä¹‰3Dç©ºé—´çº¿æ®µ")
    print("å®žæ—¶æ˜¾ç¤ºä¸¤ç‚¹é—´è·ç¦»")
    print("â†/â†’ å¿«é€€/å¿«è¿› | q: é€€å‡º")

    while True:
        current_frame_idx = cv2.getTrackbarPos('Frame', window_name)
        if current_frame_idx >= len(synced_frames):
            continue
        frame_data = synced_frames[current_frame_idx]

        # è¯»å– RGB å¸§
        rgb_frame = rgb_loader.get_frame_by_timestamp(frame_data['timestamp'])
        if rgb_frame is None:
            continue
        rgb_resized = cv2.resize(rgb_frame, target_size, interpolation=cv2.INTER_AREA)
        frame_display = cv2.resize(rgb_frame, (display_width, display_height), interpolation=cv2.INTER_AREA)
        frame_display_bgr = cv2.cvtColor(frame_display, cv2.COLOR_RGB2BGR)

        # èŽ·å–ç›¸æœºå‚æ•°
        K = np.array([[frame_data['intrinsic_row'][f'm{i}{j}'] for j in range(3)] for i in range(3)])
        orig_w, orig_h = original_rgb_size
        target_w, target_h = target_size
        fx = K[0,0] * target_w / orig_w
        fy = K[1,1] * target_h / orig_h
        cx = K[0,2] * target_w / orig_w
        cy = K[1,2] * target_h / orig_h


        T_world_to_camera = np.array([[frame_data['extrinsic_row'][f'm{i}{j}'] for j in range(4)] for i in range(4)])
        T_world_to_camera = T_world_to_camera @ T_AC
        R_world_to_camera = T_world_to_camera[:3, :3]
        t_world_to_camera = T_world_to_camera[:3, 3]

        # åæŠ•å½±ï¼šä¸–ç•Œç‚¹ â†’ å›¾åƒ
        projected_points_2d = []
        if len(world_points_3d) > 0:
            colors = [(0, 255, 0), (255, 0, 0)]
            for i, pt_world in enumerate(world_points_3d):
                pt_camera = R_world_to_camera.T @ (pt_world - t_world_to_camera)
                if pt_camera[2] <= 0:
                    projected_points_2d.append(None)
                    continue
                u_d = int((pt_camera[0] * fx / pt_camera[2]) + cx)
                v_d = int((pt_camera[1] * fy / pt_camera[2]) + cy)
                u_rgb = int(u_d / scale_rgb_to_depth_x)
                v_rgb = int(v_d / scale_rgb_to_depth_y)
                u_disp = int(u_rgb * (display_width / original_rgb_size[0]))
                v_disp = int(v_rgb * (display_height / original_rgb_size[1]))
                if 0 <= u_disp < display_width and 0 <= v_disp < display_height:
                    projected_points_2d.append((u_disp, v_disp))
                    cv2.circle(frame_display_bgr, (u_disp, v_disp), 8, colors[i], -1)
                    cv2.putText(frame_display_bgr, f'P{i+1}', (u_disp + 15, v_disp),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, colors[i], 2)
                else:
                    projected_points_2d.append(None)

            # ç”»çº¿å’Œè·ç¦»
            if len(projected_points_2d) == 2 and None not in projected_points_2d:
                cv2.line(frame_display_bgr, projected_points_2d[0], projected_points_2d[1], (255, 255, 0), 2)
                mid_x = (projected_points_2d[0][0] + projected_points_2d[1][0]) // 2
                mid_y = (projected_points_2d[0][1] + projected_points_2d[1][1]) // 2 - 10
                cv2.putText(frame_display_bgr, f'{distance_3d:.4f}m', (mid_x, mid_y),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        cv2.imshow(window_name, frame_display_bgr)
        key = cv2.waitKey(30) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('â†’') or key == 83:
            new_pos = min(current_frame_idx + 1, len(synced_frames) - 1)
            cv2.setTrackbarPos('Frame', window_name, new_pos)
        elif key == ord('â†') or key == 81:
            new_pos = max(current_frame_idx - 1, 0)
            cv2.setTrackbarPos('Frame', window_name, new_pos)

    rgb_loader.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()