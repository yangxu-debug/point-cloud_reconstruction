import os
import cv2
import numpy as np
import pandas as pd
from skimage.transform import resize
import argparse
import open3d as o3d
import json


# -------------------------------
# å·¥å…·å‡½æ•°
# -------------------------------

def read_csv_with_timestamp(csv_path):
    """è¯»å– CSVï¼Œè¿”å› timestamp -> row çš„å­—å…¸"""
    df = pd.read_csv(csv_path)
    data = {}
    for _, row in df.iterrows():
        ts = float(row['timestamp'])
        data[ts] = row
    return data


def find_closest_timestamp(target_ts, ts_list, tolerance=0.033):
    """åœ¨ ts_list ä¸­æ‰¾æœ€æ¥è¿‘ target_ts çš„æ—¶é—´æˆ³"""
    closest = min(ts_list, key=lambda ts: abs(ts - target_ts))
    if abs(closest - target_ts) < tolerance:
        return closest
    return None


def get_correction_transform():
    """ARKit åæ ‡ç³»æ ¡æ­£ï¼šç»• X è½´æ—‹è½¬ 180Â°"""
    T_AC = np.eye(4)
    T_AC[1, 1] = -1  # Y â†’ -Y
    T_AC[2, 2] = -1  # Z â†’ -Z
    return T_AC


def project_points_to_image_with_depth(vertices, intrinsic_matrix, T_world_to_camera, image_width, image_height, depth_map=None):
    """å°†3Dç‚¹æŠ•å½±åˆ°2Då›¾åƒå¹³é¢ï¼Œä½¿ç”¨æ·±åº¦å›¾è¿›è¡Œé®æŒ¡æ£€æµ‹ï¼Œè¿”å›è¯¦ç»†çš„å¯è§æ€§ä¿¡æ¯"""
    # æå–å†…å‚
    fx, fy = intrinsic_matrix[0, 0], intrinsic_matrix[1, 1]
    cx, cy = intrinsic_matrix[0, 2], intrinsic_matrix[1, 2]
    
    # ä¸–ç•Œåæ ‡åˆ°ç›¸æœºåæ ‡çš„å˜æ¢
    R = T_world_to_camera[:3, :3]
    t = T_world_to_camera[:3, 3]
    
    # å˜æ¢é¡¶ç‚¹åˆ°ç›¸æœºåæ ‡ç³»
    vertices_cam = (R @ vertices.T).T + t
    
    # åªä¿ç•™ç›¸æœºå‰æ–¹çš„ç‚¹
    valid_mask = vertices_cam[:, 2] > 0.1
    vertices_valid = vertices_cam[valid_mask]
    
    if len(vertices_valid) == 0:
        return [], [], valid_mask, [], []
    
    # æŠ•å½±åˆ°å›¾åƒå¹³é¢
    u = (fx * vertices_valid[:, 0] / vertices_valid[:, 2] + cx).astype(int)
    v = (fy * vertices_valid[:, 1] / vertices_valid[:, 2] + cy).astype(int)
    
    # è£å‰ªåˆ°å›¾åƒè¾¹ç•Œå†…
    inside_mask = (u >= 0) & (u < image_width) & (v >= 0) & (v < image_height)
    u_inside = u[inside_mask]
    v_inside = v[inside_mask]
    depths_inside = vertices_valid[inside_mask, 2]  # ç›¸æœºåæ ‡ç³»ä¸‹çš„æ·±åº¦
    
    # è·å–åŸå§‹é¡¶ç‚¹ç´¢å¼•ï¼ˆç”¨äºè·Ÿè¸ªå“ªä¸ªé¡¶ç‚¹å¯¹åº”å“ªä¸ªæŠ•å½±ç‚¹ï¼‰
    original_indices = np.where(valid_mask)[0][inside_mask]
    
    # å¦‚æœæœ‰æ·±åº¦å›¾ï¼Œè¿›è¡Œé®æŒ¡æ£€æµ‹
    if depth_map is not None:
        visible_mask = np.ones(len(u_inside), dtype=bool)
        
        for i, (u_coord, v_coord, point_depth) in enumerate(zip(u_inside, v_inside, depths_inside)):
            # è·å–æ·±åº¦å›¾ä¸­å¯¹åº”ä½ç½®çš„æ·±åº¦å€¼
            depth_from_map = depth_map[v_coord, u_coord]
            
            # å¦‚æœç‚¹äº‘ç‚¹çš„æ·±åº¦æ¯”æ·±åº¦å›¾ä¸­çš„æ·±åº¦å¤§ï¼ˆæ›´è¿œï¼‰ï¼Œåˆ™è®¤ä¸ºè¢«é®æŒ¡
            # æ·»åŠ ä¸€ä¸ªå°çš„å®¹å·®å€¼ï¼ˆæ¯”å¦‚0.1ç±³ï¼‰æ¥å¤„ç†æµ®ç‚¹è¯¯å·®
            if point_depth > depth_from_map / 1000.0 + 0.1:  # æ·±åº¦å›¾æ˜¯æ¯«ç±³ï¼Œè½¬æ¢ä¸ºç±³
                visible_mask[i] = False
        
        # åªä¿ç•™å¯è§çš„ç‚¹
        u_visible = u_inside[visible_mask]
        v_visible = v_inside[visible_mask]
        visible_indices = original_indices[visible_mask]
        
        return u_visible, v_visible, valid_mask, visible_indices, depths_inside[visible_mask]
    
    return u_inside, v_inside, valid_mask, original_indices, depths_inside


class RGBVideoLoader:
    def __init__(self, video_path, fps=60):
        self.video_path = video_path
        self.cap = cv2.VideoCapture(video_path)
        if not self.cap.isOpened():
            raise IOError(f"Cannot open video: {video_path}")
        self.fps = fps
        self.frame_count = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        print(f"Video loaded: {self.frame_count} frames at {fps} FPS")

    def get_frame_by_timestamp(self, timestamp):
        frame_idx = int(round(timestamp * self.fps))
        if frame_idx < 0 or frame_idx >= self.frame_count:
            return None
        self.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = self.cap.read()
        if not ret:
            return None
        # Rotate 90Â° CCW and convert to RGB
        #frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)
        return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    def release(self):
        self.cap.release()


# -------------------------------
# ä¸»å‡½æ•°
# -------------------------------

def main():
    parser = argparse.ArgumentParser(description="Convert custom dataset to ScanNet 2D format with textured mesh and debug projection.")
    parser.add_argument('--input_dir', default='20250820-212141', help='è¾“å…¥ç›®å½•ï¼ŒåŒ…å« depth/, video.mov, intrinsics.csv, extrinsics.csv, mesh.ply')
    parser.add_argument('--output_path', default='output', help='è¾“å‡ºæ ¹ç›®å½•')
    parser.add_argument('--scene_name', default='scene0000_00', help='è¾“å‡ºåœºæ™¯åç§°')
    parser.add_argument('--frame_skip', type=int, default=30, help='æ¯éš”å‡ å¸§å¤„ç†ä¸€æ¬¡ï¼ˆ1 è¡¨ç¤ºå…¨å–ï¼‰')
    parser.add_argument('--output_image_width', type=int, default=1920, help='è¾“å‡ºå›¾åƒå®½åº¦')
    parser.add_argument('--output_image_height', type=int, default=1440, help='è¾“å‡ºå›¾åƒé«˜åº¦')
    parser.add_argument('--original_rgb_width', type=int, default=1920, help='åŸå§‹ RGB å›¾åƒå®½åº¦')
    parser.add_argument('--original_rgb_height', type=int, default=1440, help='åŸå§‹ RGB å›¾åƒé«˜åº¦')
    parser.add_argument('--tolerance', type=float, default=0.033, help='æ—¶é—´å¯¹é½å®¹å¿åº¦ï¼ˆç§’ï¼‰')
    parser.add_argument('--export_label_images', action='store_true', help='æ˜¯å¦å¯¼å‡ºæ ‡ç­¾å›¾åƒï¼ˆæœ¬æ•°æ®é›†é€šå¸¸æ²¡æœ‰ï¼‰')
    parser.add_argument('--debug_projected_points', default=True,
                        help='è°ƒè¯•æ¨¡å¼ï¼šå°† mesh é¡¶ç‚¹æŠ•å½±åˆ°å›¾åƒå¹¶ä¿å­˜å¯è§†åŒ–ç»“æœï¼ˆç”¨äºæ£€æŸ¥å†…å‚/å¤–å‚æ˜¯å¦æ­£ç¡®ï¼‰')
    args = parser.parse_args()

    base_dir = args.input_dir
    output_dir = os.path.join(args.output_path, args.scene_name)
    os.makedirs(output_dir, exist_ok=True)

    # åˆ›å»ºè¾“å‡ºå­ç›®å½•
    color_dir = os.path.join(output_dir, 'color')
    depth_dir = os.path.join(output_dir, 'depth')
    pose_dir = os.path.join(output_dir, 'pose')
    os.makedirs(color_dir, exist_ok=True)
    os.makedirs(depth_dir, exist_ok=True)
    os.makedirs(pose_dir, exist_ok=True)

    if args.export_label_images:
        label_dir = os.path.join(output_dir, 'label')
        os.makedirs(label_dir, exist_ok=True)

  
    intrinsics_csv = os.path.join(base_dir, 'intrinsics_log.csv')
    extrinsics_csv = os.path.join(base_dir, 'extrinsics_log.csv')

    if not os.path.exists(intrinsics_csv):
        print(f"âŒ Intrinsics file not found: {intrinsics_csv}")
        return
    if not os.path.exists(extrinsics_csv):
        print(f"âŒ Extrinsics file not found: {extrinsics_csv}")
        return

    intrinsics_dict = read_csv_with_timestamp(intrinsics_csv)
    extrinsics_dict = read_csv_with_timestamp(extrinsics_csv)

    # -------------------------------
    # 3. åŠ è½½ RGB è§†é¢‘
    # -------------------------------
    video_path = os.path.join(base_dir, 'video.mov')
    if not os.path.exists(video_path):
        print(f"âŒ RGB video not found: {video_path}")
        return

    rgb_loader = RGBVideoLoader(video_path, fps=60)

    # -------------------------------
    # 4. æ—¶é—´å¯¹é½å¹¶ç­›é€‰æœ‰æ•ˆå¸§
    # -------------------------------
    sorted_ts = sorted(extrinsics_dict.keys())
    valid_entries = []

    for idx, depth_ts in enumerate(sorted_ts):
        if idx % args.frame_skip != 0:
            continue

        closest_intrinsic_ts = find_closest_timestamp(depth_ts, list(intrinsics_dict.keys()), args.tolerance)
        closest_extrinsic_ts = find_closest_timestamp(depth_ts, list(extrinsics_dict.keys()), args.tolerance)

        if closest_intrinsic_ts is None or closest_extrinsic_ts is None:
            continue

        valid_entries.append({
            'depth_ts': depth_ts,
            'intrinsic_ts': closest_intrinsic_ts,
            'extrinsic_ts': closest_extrinsic_ts,
        })

    if not valid_entries:
        print("âŒ No synchronized frames found.")
        return
    print(f"âœ… {len(valid_entries)} frames will be exported.")

    # è®¾ç½®ç›®æ ‡å›¾åƒå°ºå¯¸
    target_size = (args.output_image_width, args.output_image_height)
    
    # åŠ è½½ç‚¹äº‘
    mesh_path = os.path.join(base_dir, 'mesh.ply')
    if not os.path.exists(mesh_path):
        print(f"âŒ Mesh file not found: {mesh_path}")
        return
    
    print(f"ğŸ¨ Loading mesh from {mesh_path}...")
    mesh = o3d.io.read_triangle_mesh(mesh_path)
    if len(mesh.vertices) == 0:
        print("âš ï¸ Empty mesh, skipping.")
        return
    
    vertices = np.asarray(mesh.vertices)
    print(f"âœ… Loaded {len(vertices)} vertices")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    debug_dir = os.path.join(output_dir, 'debug_projected')
    os.makedirs(debug_dir, exist_ok=True)
    
    # åˆå§‹åŒ–é¡¶ç‚¹å¯è§æ€§è·Ÿè¸ªæ•°æ®ç»“æ„
    vertex_visibility_data = {
        'mesh_info': {
            'total_vertices': len(vertices),
            'frame_count': len(valid_entries)
        },
        'vertices': {}  # æŒ‰é¡¶ç‚¹ç´¢å¼•ç»„ç»‡æ•°æ®
    }
    
    # åˆå§‹åŒ–æ¯ä¸ªé¡¶ç‚¹çš„æ•°æ®ç»“æ„
    for i in range(len(vertices)):
        vertex_visibility_data['vertices'][str(i)] = {
            'vertex_index': i,
            'world_coords': vertices[i].tolist(),
            'visible_frames': []  # è®°å½•åœ¨å“ªäº›å¸§ä¸­å¯è§
        }
    
    # åˆå§‹åŒ–ç›¸æœºå‚æ•°æ•°æ®ç»“æ„
    camera_params_data = {
        'mesh_info': {
            'total_vertices': len(vertices),
            'frame_count': len(valid_entries)
        },
        'frames': []  # æŒ‰å¸§ç»„ç»‡ç›¸æœºå‚æ•°
    }
    
    # ç›´æ¥å¤„ç†æ¯ä¸€å¸§çš„ç‚¹äº‘æŠ•å½±
    frame_index = 0
    successful_projections = 0
    
    for entry in valid_entries:
        depth_ts = entry['depth_ts']
        print(f"\nå¤„ç†ç¬¬ {frame_index + 1}/{len(valid_entries)} å¸§ (æ—¶é—´æˆ³: {depth_ts:.3f})")
        
        # 1. è·å–RGBå¸§
        rgb_frame = rgb_loader.get_frame_by_timestamp(depth_ts)
        if rgb_frame is None:
            print(f"  âš ï¸ æ— æ³•è·å–RGBå¸§ï¼Œè·³è¿‡")
            frame_index += 1
            continue
        
        # è°ƒæ•´RGBå¸§å°ºå¯¸
        rgb_resized = cv2.resize(rgb_frame, target_size, interpolation=cv2.INTER_LINEAR)
        
        # 2. è·å–ç›¸æœºå‚æ•°
        intrinsic_row = intrinsics_dict[entry['intrinsic_ts']]
        extrinsic_row = extrinsics_dict[entry['extrinsic_ts']]
        
        # æ„å»ºå†…å‚çŸ©é˜µ
        intrinsic_matrix = np.array([
            [intrinsic_row['m00'], 0, intrinsic_row['m02']],
            [0, intrinsic_row['m11'], intrinsic_row['m12']],
            [0, 0, 1]
        ])
        
        # æ„å»ºå¤–å‚çŸ©é˜µ
        extrinsic_matrix = np.array([
            [extrinsic_row['m00'], extrinsic_row['m01'], extrinsic_row['m02'], extrinsic_row['m03']],
            [extrinsic_row['m10'], extrinsic_row['m11'], extrinsic_row['m12'], extrinsic_row['m13']],
            [extrinsic_row['m20'], extrinsic_row['m21'], extrinsic_row['m22'], extrinsic_row['m23']],
            [extrinsic_row['m30'], extrinsic_row['m31'], extrinsic_row['m32'], extrinsic_row['m33']]
        ])
        
        # åº”ç”¨ARKitæ ¡æ­£
        T_AC = get_correction_transform()
        extrinsic_matrix = extrinsic_matrix @ T_AC
        T_world_to_camera = np.linalg.inv(extrinsic_matrix)
        
        # 3. åŠ è½½æ·±åº¦å›¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        depth_file_pattern = f"{depth_ts:.6f}_W256_H192_DepthFloat32.raw"
        depth_dir_path = os.path.join(base_dir, 'depth')
        depth_file_path = os.path.join(depth_dir_path, depth_file_pattern)
        
        depth_map = None
        if os.path.exists(depth_file_path):
            try:
                # åŠ è½½æ·±åº¦å›¾
                depth_data = np.fromfile(depth_file_path, dtype=np.float32)
                depth_img = depth_data.reshape((192, 256))
                
                # è°ƒæ•´æ·±åº¦å›¾å°ºå¯¸ä»¥åŒ¹é…è¾“å‡ºå›¾åƒ
                depth_resized = resize(depth_img, target_size[::-1], order=0, preserve_range=True, anti_aliasing=False)
                
                # è½¬æ¢ä¸ºæ¯«ç±³å•ä½
                depth_map = (depth_resized * 1000).astype(np.float32)
                print(f"  âœ… æ·±åº¦å›¾å·²åŠ è½½: {depth_map.shape}")
            except Exception as e:
                print(f"  âš ï¸ æ·±åº¦å›¾åŠ è½½å¤±è´¥: {e}")
        else:
            print(f"  âš ï¸ æœªæ‰¾åˆ°æ·±åº¦å›¾: {depth_file_pattern}")
        
        # 4. æ‰§è¡Œç‚¹äº‘æŠ•å½±
        print(f"  ğŸ” æ‰§è¡Œç‚¹äº‘æŠ•å½±...")
        u_proj, v_proj, valid_mask, visible_indices, depths = project_points_to_image_with_depth(
            vertices, intrinsic_matrix, T_world_to_camera, 
            target_size[0], target_size[1], depth_map
        )
        
        if len(u_proj) > 0:
            # è®°å½•æ¯ä¸ªå¯è§é¡¶ç‚¹çš„ä¿¡æ¯
            for i, (u, v, vertex_idx, depth) in enumerate(zip(u_proj, v_proj, visible_indices, depths)):
                # å°†å¯è§æ€§ä¿¡æ¯æ·»åŠ åˆ°å¯¹åº”é¡¶ç‚¹çš„è®°å½•ä¸­
                vertex_key = str(vertex_idx)
                frame_info = {
                    'frame_index': frame_index,
                    'timestamp': depth_ts,
                    'image_coords': [int(u), int(v)],
                    'depth': float(depth),
                    'intrinsic_matrix': intrinsic_matrix.tolist()  # ä¿å­˜å†…å‚çŸ©é˜µ
                }
                vertex_visibility_data['vertices'][vertex_key]['visible_frames'].append(frame_info)
            
            # 5. å¯è§†åŒ–æŠ•å½±ç»“æœ
            rgb_debug = rgb_resized.copy()
            
            # åœ¨å›¾åƒä¸Šç”»æŠ•å½±ç‚¹
            for u, v in zip(u_proj, v_proj):
                cv2.circle(rgb_debug, (u, v), radius=1, color=(255, 0, 0), thickness=-1)
            
            # ä¿å­˜ç»“æœ
            output_path = os.path.join(debug_dir, f"frame_{frame_index:04d}_ts_{depth_ts:.3f}.jpg")
            cv2.imwrite(output_path, cv2.cvtColor(rgb_debug, cv2.COLOR_RGB2BGR))
            
            print(f"  âœ… æŠ•å½±æˆåŠŸ: {len(u_proj)} ä¸ªç‚¹ -> {output_path}")
            successful_projections += 1
        else:
            print(f"  âš ï¸ æ²¡æœ‰æœ‰æ•ˆæŠ•å½±ç‚¹")
        
        # å°†è¿™ä¸€å¸§çš„ç›¸æœºå‚æ•°æ·»åŠ åˆ°ç›¸æœºå‚æ•°æ•°æ®ä¸­
        frame_camera_info = {
            'frame_index': frame_index,
            'timestamp': depth_ts,
            'intrinsic_matrix': intrinsic_matrix.tolist(),
            'T_world_to_camera': T_world_to_camera.tolist()
        }
        camera_params_data['frames'].append(frame_camera_info)
        
        frame_index += 1
    
    print(f"\n=== å¤„ç†å®Œæˆ ===")
    print(f"   æ€»å¸§æ•°: {len(valid_entries)}")
    print(f"   æˆåŠŸæŠ•å½±: {successful_projections} å¸§")
    print(f"   ç»“æœä¿å­˜åœ¨: {debug_dir}/")
    
    # ä¿å­˜é¡¶ç‚¹å¯è§æ€§æ•°æ®åˆ°JSONæ–‡ä»¶
    json_output_path = os.path.join(output_dir, 'vertex_visibility_data.json')
    with open(json_output_path, 'w', encoding='utf-8') as f:
        json.dump(vertex_visibility_data, f, indent=2, ensure_ascii=False)
    
    print(f"   é¡¶ç‚¹å¯è§æ€§æ•°æ®ä¿å­˜åˆ°: {json_output_path}")
    
    # ä¿å­˜ç›¸æœºå‚æ•°æ•°æ®åˆ°JSONæ–‡ä»¶
    camera_params_path = os.path.join(output_dir, 'camera_params_data.json')
    with open(camera_params_path, 'w', encoding='utf-8') as f:
        json.dump(camera_params_data, f, indent=2, ensure_ascii=False)
    
    print(f"   ç›¸æœºå‚æ•°æ•°æ®ä¿å­˜åˆ°: {camera_params_path}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_visible_vertices = sum(len(vertex_data['visible_frames']) for vertex_data in vertex_visibility_data['vertices'].values())
    print(f"   æ€»å¯è§é¡¶ç‚¹æ¬¡æ•°: {total_visible_vertices}")
    
    # è®¡ç®—æ¯ä¸ªé¡¶ç‚¹çš„å¯è§æ€§ç»Ÿè®¡
    vertex_visibility_count = {}
    for vertex_key, vertex_data in vertex_visibility_data['vertices'].items():
        frame_count = len(vertex_data['visible_frames'])
        if frame_count > 0:
            vertex_visibility_count[vertex_key] = frame_count
    
    if vertex_visibility_count:
        max_visibility = max(vertex_visibility_count.values())
        min_visibility = min(vertex_visibility_count.values())
        avg_visibility = sum(vertex_visibility_count.values()) / len(vertex_visibility_count)
        print(f"   é¡¶ç‚¹å¯è§æ€§ç»Ÿè®¡:")
        print(f"     æœ€å¤šå¯è§å¸§æ•°: {max_visibility}")
        print(f"     æœ€å°‘å¯è§å¸§æ•°: {min_visibility}")
        print(f"     å¹³å‡å¯è§å¸§æ•°: {avg_visibility:.1f}")
        print(f"     æœ‰å¯è§è®°å½•çš„é¡¶ç‚¹æ•°: {len(vertex_visibility_count)}")
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹é¡¶ç‚¹çš„å¯è§æ€§ä¿¡æ¯
        print(f"\n   ç¤ºä¾‹é¡¶ç‚¹å¯è§æ€§ä¿¡æ¯:")
        sample_vertices = list(vertex_visibility_count.keys())[:3]  # æ˜¾ç¤ºå‰3ä¸ªé¡¶ç‚¹
        for vertex_key in sample_vertices:
            vertex_data = vertex_visibility_data['vertices'][vertex_key]
            print(f"     é¡¶ç‚¹ {vertex_key}: åœ¨ {len(vertex_data['visible_frames'])} å¸§ä¸­å¯è§")
            if vertex_data['visible_frames']:
                first_frame = vertex_data['visible_frames'][0]
                print(f"       ç¤ºä¾‹: å¸§ {first_frame['frame_index']}, åæ ‡ {first_frame['image_coords']}")

    # -------------------------------
    # æ¸…ç†èµ„æº
    # -------------------------------
    rgb_loader.release()
    print("âœ… All done!")


if __name__ == "__main__":
    main()